<h1 align="center">
  ğŸ¥‹ UFC Fight Predictor v2
  <img src="img/ufc_logo.png" width="70" style="vertical-align: middle; margin-left: 10px;" />
</h1>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11-blue"/>
  <img src="https://img.shields.io/badge/license-MIT-blue"/>
  <img src="https://img.shields.io/badge/docker-ready-blue"/>
</p>

## ğŸ“ Project Summary
UFC Fight Predictor is a machine learning pipeline developed with AutoGluon to predict the outcomes of UFC fights by combining fighter statistics, performance history, and betting market signals.

---

> Check UFC Fight Predictor v1.0.

<p align="center">
  <img src="img/ufc_sh.gif" alt="UFC CLI Demo" width="85%" />
</p>

---

## ğŸ¯ Objective

UFC Fight Predictor v2 aims to build a robust **binary classification model** to predict the winner of a UFC fight.  
This version leverages **AutoGluon** for automated model selection and hyperparameter tuning, allowing for stronger baseline performance compared to the v1 models.

The pipeline combines detailed fighter and performance statistics with a custom **UFCStats scraper**, ensuring continuous updates after every UFC event.

---

## ğŸ“Š Dataset Description

### v2 Dataset (Current)

The updated dataset includes **over 8,000 UFC fights (2010â€“2025)** sourced from UFCStats.  
Each row represents a single bout with detailed per-fighter statistics, performance metrics, and fight context.

#### Key Feature Categories
- ğŸ§ **Fighter Attributes**  
  Height, reach, weight, stance, age, professional record.  
- ğŸ¯ **Performance Metrics**  
  - Strikes landed/attempted by target area (head, body, legs).  
  - Striking accuracy and defense rates.  
  - Takedown attempts, accuracy, and defense.  
  - Submission attempts and control time.  
- ğŸ† **Fight Context**  
  Event, location, date, division, rounds, title fight status.  
- âš¡ **Target Variable**  
  - **0** â†’ Red Corner Win  
  - **1** â†’ Blue Corner Win  

---

âš™ï¸ With this enriched dataset and AutoGluon integration, UFC Fight Predictor v2 delivers **improved accuracy and adaptability** for real-time UFC fight predictions.

---

## ğŸ› ï¸ Modeling Approach

The modeling pipeline is structured into three interconnected stages, designed to maximize predictive performance while ensuring interpretability and robustness, all preprocessing, feature engineering, and data splitting is handled via the modular UFCData class, ensuring consistent transformations across training and evaluation. All models are wrapped and evaluated through the UFCModel class.

1. **ğŸ”§ Feature Engineering**

   - A synthetic random noise feature (`Random_Noise`) is introduced as a baseline to assess feature importance. Different combinations were explored until the random column gained prominence, guiding the final selection. This iterative process resulted in a feature set that balances complexity, interpretability, and predictive power.

2. **ğŸ¤– Model Training**

   - The task is framed as a binary classification problem, with a baseline distribution of approximately 58% red corner wins, reflecting historical outcome imbalance.

3. **ğŸ“Š Evaluation**
   - Model performance is assessed using a comprehensive set of metrics, computed via the modular `metrics.py` implementation:
     - **Accuracy** (0â€“1, higher is better): Overall proportion of correct predictions.
     - **Precision** (0â€“1, higher is better): Share of positive predictions that are actually correct.
     - **Recall** (0â€“1, higher is better): Share of true positives correctly identified.
     - **F1 Score** (0â€“1, higher is better): Harmonic mean of precision and recall, balancing both.
     - **ROC-AUC** (0.5â€“1, higher is better): Probability that the model ranks a random positive higher than a random negative.
     - **Brier Score** (0â€“1, lower is better): Mean squared error between predicted probabilities and actual outcomes, reflecting calibration.
   - Confusion matrices are used to visualize classification performance across true and false positives and negatives.
   - The framework supports automated multi-model comparison, enabling the identification of top-performing models per metric and facilitating robust benchmarking.

---

## ğŸ¤– Models Implemented

## ğŸ§  Feature Importance Analysis (With vs. Without Odds)

A comparative analysis of feature importance across models trained **with** and **without** betting odds reveals key shifts in predictive behavior.

### ğŸ” Models Trained Without Odds

---

### ğŸ” Models Trained With Odds

---

### ğŸ§© Conclusion

- Without odds, models must infer advantage purely from physical and statistical differences between fighters.
- With odds, models gain access to a **powerful proxy of market knowledge**, which reflects public perception, fighter form, injury rumors, and expert insightsâ€”all aggregated into a single feature.
- This results in higher predictive accuracy and more calibrated outputs, but also **reduces model reliance on handcrafted features**.

> Betting odds act as a real-world prior, dramatically enhancing model confidenceâ€”but at the cost of reduced interpretability and generalization when odds are unavailable.

## ğŸ“Š Model Performance Summary

## ğŸ“ˆ Model Performance Summary (No Odds)

> ğŸ“Œ *Complete results and additional visualizations can be inspected in `notebooks/05-model_experiments.ipynb`.*

---

### ğŸ“Š Metrics Analysis and Predictive Limits

## ğŸ§© Feature Descriptions

## ğŸ”¬ Noise-Based Feature Selection

To improve feature selection, we conducted a **Noise-Based Feature Selection** experiment. A synthetic random feature (`Random_Noise`) was added to the dataset using `UFCData.add_random_feature()`, and feature importance was analyzed across multiple models. Any real feature showing lower importance than the random column was considered uninformative and a candidate for exclusion.

This iterative process helped refine the feature set, striking a balance between **model complexity, interpretability, and predictive performance**.  
**Below: on the left, feature importances with the random noise benchmark; on the right, after applying several feature engineering refinements, with the random noise column removed:**

---

ğŸ› ï¸ğŸš§ Under Construction

## ğŸš€ Getting Started

You can interact with UFC Fight Predictor v2 in two ways:

---

### ğŸ§ª Run the pipeline via notebooks

1. **Clone the repository**

```bash
git clone https://github.com/mfourier/ufc-predictor-v2.git
cd ufc-predictor
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```

3. **Run the pipeline notebooks**

Follow the workflow step by step:

- `notebooks/01-etl.ipynb` â†’ Data cleaning and preparation  
- `notebooks/02-eda.ipynb` â†’ Exploratory data analysis  
- `notebooks/03-feature_engineering.ipynb` â†’ Feature construction  
- `notebooks/04-training.ipynb` â†’ Model training and tuning  
- `notebooks/05-model_experiments.ipynb` â†’ Evaluation and comparison  

---

## ğŸ§ª Project Structure

```bash
ufc-predictor/
â”œâ”€â”€ app.py                            # Main entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original fight data
â”‚   â”œâ”€â”€ processed/                    # Cleaned and transformed datasets
â”‚   â””â”€â”€ results/                      # Evaluation logs, metrics, model reports
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-etl.ipynb                  # Data extraction and cleaning
â”‚   â”œâ”€â”€ 02-eda.ipynb                  # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 03-feature_engineering.ipynb  # Feature engineering using UFCData
â”‚   â”œâ”€â”€ 04-training.ipynb             # Model training using the training set
â”‚   â”œâ”€â”€ 05-model_experiments.ipynb    # Model comparison and results analysis
â”‚   â””â”€â”€ 06-deployment.ipynb           # Deployment exploration and integration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                     # Model hyperparameters and registry
â”‚   â”œâ”€â”€ data.py                       # UFCData class: manages data splits and transformations
â”‚   â”œâ”€â”€ helpers.py                    # Utility and preprocessing functions
â”‚   â”œâ”€â”€ io_model.py                   # Save/load model objects from disk
â”‚   â”œâ”€â”€ metrics.py                    # Evaluation metrics and plots
â”‚   â”œâ”€â”€ model.py                      # UFCModel class: Wrapper class for saving, loading, and evaluating models
â”‚   â”œâ”€â”€ model_factory.py              # Central model selection logic
â”‚   â””â”€â”€ predictor.py                  # UFCPredictor class: interactive fight prediction interface
â”œâ”€â”€ docs/                             # Markdown documentation per model
â”œâ”€â”€ img/                              # Images for plots, logos, and visuals
â””â”€â”€ requirements.txt                  # Project dependencies

```

---

## ğŸ“š Documentation

Comprehensive project documentation is available in the `docs/` folder, covering:

- **Model overviews and mathematical formulations**: Detailed descriptions of each algorithm, including underlying principles and expected behavior.
- **Key assumptions and limitations**: Insights into when and why each model performs best, as well as potential pitfalls.
- **Training logs**: A CSV file automatically generated during experiments, storing key metrics, best hyperparameters, and training durations for each model, enabling result tracking and comparison across runs.
- **Usage guides**: Step-by-step instructions on running the notebooks, customizing experiments, and interpreting results.

---

## ğŸ‘¥ Contributors

- **Maximiliano Lioi** â€” M.Sc. in Applied Mathematics @ University of Chile
- **RocÃ­o YÃ¡Ã±ez** â€” M.Sc. in Applied Mathematics @ University of Chile

---

### Disclaimer

This project is an independent work for academic and research purposes.  
It is not affiliated with, endorsed by, or sponsored by UFC, Zuffa LLC, or any related entity.  
All trademarks and fight data belong to their respective owners.

